{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aab851e",
   "metadata": {},
   "source": [
    "## Part 1: Evaluating Harmful Associations in Static Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01eccc",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "The goal of this part is to evaluate harmful word associations in static word embeddings like Word2Vec or GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617eb829",
   "metadata": {},
   "source": [
    "# Key Steps:\n",
    "\n",
    "**Load Pre-trained Word Embeddings**: Load pre-trained word embeddings such as GloVe or Word2Vec.\n",
    "\n",
    "**Identify Harmful Associations**: Detect and evaluate harmful associations, such as: \\\n",
    "    Gender bias (e.g., \"doctor\" vs \"nurse\" or \"man\" vs \"woman\")\\\n",
    "    Racial bias (e.g., associations related to ethnicity or nationality)\n",
    "\n",
    "**Quantify Harmful Associations**: Use cosine similarity between word vectors to measure biased associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6618ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3000000, 300)\n",
      "'International' not found in the embeddings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the word2vec embeddings\n",
    "embedding_model_path = r\"D:\\RESEARCH related\\PreCog tasks\\Language_representations\\models\\word2vec.npy\"\n",
    "embeddings = np.load(embedding_model_path, allow_pickle=True)\n",
    "\n",
    "# Check the type and structure of the loaded embeddings\n",
    "print(type(embeddings))  # To verify what kind of object you have loaded\n",
    "print(embeddings.shape if hasattr(embeddings, 'shape') else \"No shape attribute\")\n",
    "\n",
    "# Retrieve the embedding for the word 'International'\n",
    "word = \"International\"\n",
    "embedding = embeddings.get(word) if isinstance(embeddings, dict) else None\n",
    "\n",
    "if embedding is not None:\n",
    "    print(f\"Embedding for '{word}':\")\n",
    "    print(embedding)\n",
    "else:\n",
    "    print(f\"'{word}' not found in the embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bf13c",
   "metadata": {},
   "source": [
    "# Evaluate Gender Bias: \n",
    "Gender bias in word embeddings can be quantified by measuring the difference between words typically associated with each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04f7f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m profession \u001b[38;5;129;01min\u001b[39;00m professions:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m gender \u001b[38;5;129;01min\u001b[39;00m gendered_words:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         similarity = \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     20\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSimilarity between \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgender\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofession\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcalculate_similarity\u001b[39m\u001b[34m(word1, word2, embeddings)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_similarity\u001b[39m(word1, word2, embeddings):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     vec1 = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(word1)\n\u001b[32m     10\u001b[39m     vec2 = embeddings.get(word2)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m vec1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m vec2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Gendered words and gender-associated professions\n",
    "gendered_words = ['man', 'woman']\n",
    "professions = ['doctor', 'nurse', 'teacher']\n",
    "\n",
    "# Function to calculate the cosine similarity\n",
    "def calculate_similarity(word1, word2, embeddings):\n",
    "    vec1 = embeddings.get(word1)\n",
    "    vec2 = embeddings.get(word2)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        return 1 - cosine(vec1, vec2)  # Cosine similarity\n",
    "    return None\n",
    "\n",
    "# Calculate similarity between gendered words and professions\n",
    "for profession in professions:\n",
    "    for gender in gendered_words:\n",
    "        similarity = calculate_similarity(gender, profession, embeddings)\n",
    "        if similarity is not None:\n",
    "            print(f\"Similarity between '{gender}' and '{profession}': {similarity}\")\n",
    "        else:\n",
    "            print(f\"Embedding for '{gender}' or '{profession}' not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43511d3f",
   "metadata": {},
   "source": [
    "**Quantifying Bias:** To quantify the bias, compute the difference in similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483817d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate bias for \"man\" vs \"woman\" and their association with \"doctor\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m similarity_man_doctor = \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mman\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdoctor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m similarity_woman_doctor = calculate_similarity(\u001b[33m'\u001b[39m\u001b[33mwoman\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdoctor\u001b[39m\u001b[33m'\u001b[39m, embeddings)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Compute the bias score\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcalculate_similarity\u001b[39m\u001b[34m(word1, word2, embeddings)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_similarity\u001b[39m(word1, word2, embeddings):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     vec1 = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(word1)\n\u001b[32m     10\u001b[39m     vec2 = embeddings.get(word2)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m vec1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m vec2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Calculate bias for \"man\" vs \"woman\" and their association with \"doctor\"\n",
    "similarity_man_doctor = calculate_similarity('man', 'doctor', embeddings)\n",
    "similarity_woman_doctor = calculate_similarity('woman', 'doctor', embeddings)\n",
    "\n",
    "# Compute the bias score\n",
    "if similarity_man_doctor is not None and similarity_woman_doctor is not None:\n",
    "    bias_score = similarity_man_doctor - similarity_woman_doctor\n",
    "    print(f\"Bias score (man vs woman and doctor): {bias_score}\")\n",
    "else:\n",
    "    print(\"Embeddings for 'man' or 'woman' or 'doctor' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bea703",
   "metadata": {},
   "source": [
    "#### Visualize Harmful Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca664a40",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m words = [\u001b[33m'\u001b[39m\u001b[33mking\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mqueen\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mman\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwoman\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdoctor\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnurse\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Retrieve word vectors for these words\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m word_vectors = [\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Apply PCA to reduce dimensionality to 2D\u001b[39;00m\n\u001b[32m     11\u001b[39m pca = PCA(n_components=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# List of words to visualize\n",
    "words = ['king', 'queen', 'man', 'woman', 'doctor', 'nurse']\n",
    "\n",
    "# Retrieve word vectors for these words\n",
    "word_vectors = [embeddings.get(word) for word in words]\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 2D\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the word vectors\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (pca_result[i, 0], pca_result[i, 1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a109f3",
   "metadata": {},
   "source": [
    "### Key Insights:\n",
    "This part will show if your static word embeddings contain any harmful biases, based on the evaluation of similarity between gendered roles or ethnic associations.\n",
    "\n",
    "If harmful associations are present, you can discuss strategies to reduce these biases such as debiasing algorithms like Hard Debiasing or Soft Debiasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a055d",
   "metadata": {},
   "source": [
    "# Part 2: Analyzing Harmful Biases in Contextual Embeddings (e.g., BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601bc8c",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "The goal here is to analyze harmful biases in contextual embeddings, such as those produced by BERT, which take the surrounding context into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0076d6",
   "metadata": {},
   "source": [
    "# Key Steps:\n",
    "**Use Pre-Trained BERT Embeddings**: Load a pre-trained BERT model using transformers library.\n",
    "\n",
    "**Perform Bias Analysis on Contextual Embeddings**: Evaluate harmful associations such as gender bias, racial bias, and occupation bias in contextual embeddings.\n",
    "\n",
    "**Quantify Bias in Contextual Embeddings**: Measure how different words' embeddings change in different contexts (e.g., \"he is a doctor\" vs. \"she is a doctor\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9bab32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: colorama in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 0.5/10.4 MB 69.3 kB/s eta 0:02:23\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 0.8/10.4 MB 79.9 kB/s eta 0:02:01\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 91.3 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 92.7 kB/s eta 0:01:38\n",
      "   ------ --------------------------------- 1.6/10.4 MB 100.3 kB/s eta 0:01:28\n",
      "   ------ --------------------------------- 1.6/10.4 MB 100.3 kB/s eta 0:01:28\n",
      "   ------ --------------------------------- 1.6/10.4 MB 100.3 kB/s eta 0:01:28\n",
      "   ------- -------------------------------- 1.8/10.4 MB 114.8 kB/s eta 0:01:15\n",
      "   ------- -------------------------------- 1.8/10.4 MB 114.8 kB/s eta 0:01:15\n",
      "   ------- -------------------------------- 1.8/10.4 MB 114.8 kB/s eta 0:01:15\n",
      "   ------- -------------------------------- 1.8/10.4 MB 114.8 kB/s eta 0:01:15\n",
      "   ------- -------------------------------- 1.8/10.4 MB 114.8 kB/s eta 0:01:15\n",
      "   -------- ------------------------------- 2.1/10.4 MB 124.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 2.1/10.4 MB 124.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 2.1/10.4 MB 124.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 2.1/10.4 MB 124.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 2.1/10.4 MB 124.0 kB/s eta 0:01:07\n",
      "   -------- ------------------------------- 2.1/10.4 MB 124.0 kB/s eta 0:01:07\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   --------- ------------------------------ 2.4/10.4 MB 131.6 kB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 129.8 kB/s eta 0:01:00\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 133.4 kB/s eta 0:00:57\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------ --------------------------- 3.1/10.4 MB 129.3 kB/s eta 0:00:56\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 3.4/10.4 MB 127.3 kB/s eta 0:00:55\n",
      "   -------------- ------------------------- 3.7/10.4 MB 131.1 kB/s eta 0:00:52\n",
      "   -------------- ------------------------- 3.7/10.4 MB 131.1 kB/s eta 0:00:52\n",
      "   -------------- ------------------------- 3.7/10.4 MB 131.1 kB/s eta 0:00:52\n",
      "   -------------- ------------------------- 3.7/10.4 MB 131.1 kB/s eta 0:00:52\n",
      "   -------------- ------------------------- 3.7/10.4 MB 131.1 kB/s eta 0:00:52\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 3.9/10.4 MB 135.4 kB/s eta 0:00:48\n",
      "   ---------------- ----------------------- 4.2/10.4 MB 134.1 kB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 4.2/10.4 MB 134.1 kB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 4.2/10.4 MB 134.1 kB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 4.2/10.4 MB 134.1 kB/s eta 0:00:47\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 149.2 kB/s eta 0:00:40\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 4.7/10.4 MB 149.7 kB/s eta 0:00:38\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 5.0/10.4 MB 153.7 kB/s eta 0:00:36\n",
      "   -------------------- ------------------- 5.2/10.4 MB 149.9 kB/s eta 0:00:35\n",
      "   -------------------- ------------------- 5.2/10.4 MB 149.9 kB/s eta 0:00:35\n",
      "   -------------------- ------------------- 5.2/10.4 MB 149.9 kB/s eta 0:00:35\n",
      "   -------------------- ------------------- 5.2/10.4 MB 149.9 kB/s eta 0:00:35\n",
      "   -------------------- ------------------- 5.2/10.4 MB 149.9 kB/s eta 0:00:35\n",
      "   -------------------- ------------------- 5.2/10.4 MB 149.9 kB/s eta 0:00:35\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 5.5/10.4 MB 153.3 kB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 157.6 kB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 157.5 kB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 157.5 kB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 157.5 kB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 157.5 kB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 157.5 kB/s eta 0:00:28\n",
      "   ------------------------ --------------- 6.3/10.4 MB 160.9 kB/s eta 0:00:26\n",
      "   ------------------------ --------------- 6.3/10.4 MB 160.9 kB/s eta 0:00:26\n",
      "   ------------------------ --------------- 6.3/10.4 MB 160.9 kB/s eta 0:00:26\n",
      "   ------------------------ --------------- 6.3/10.4 MB 160.9 kB/s eta 0:00:26\n",
      "   ------------------------ --------------- 6.3/10.4 MB 160.9 kB/s eta 0:00:26\n",
      "   ------------------------ --------------- 6.3/10.4 MB 160.9 kB/s eta 0:00:26\n",
      "   ------------------------- -------------- 6.6/10.4 MB 158.3 kB/s eta 0:00:25\n",
      "   ------------------------- -------------- 6.6/10.4 MB 158.3 kB/s eta 0:00:25\n",
      "   ------------------------- -------------- 6.6/10.4 MB 158.3 kB/s eta 0:00:25\n",
      "   ------------------------- -------------- 6.6/10.4 MB 158.3 kB/s eta 0:00:25\n",
      "   ------------------------- -------------- 6.6/10.4 MB 158.3 kB/s eta 0:00:25\n",
      "   ------------------------- -------------- 6.6/10.4 MB 158.3 kB/s eta 0:00:25\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   -------------------------- ------------- 6.8/10.4 MB 153.8 kB/s eta 0:00:24\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   --------------------------- ------------ 7.1/10.4 MB 154.5 kB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 7.3/10.4 MB 154.6 kB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 7.3/10.4 MB 154.6 kB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 7.3/10.4 MB 154.6 kB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 7.3/10.4 MB 154.6 kB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 159.7 kB/s eta 0:00:18\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 7.9/10.4 MB 164.7 kB/s eta 0:00:16\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 8.1/10.4 MB 169.6 kB/s eta 0:00:14\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 8.4/10.4 MB 167.2 kB/s eta 0:00:12\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 8.7/10.4 MB 162.9 kB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 160.6 kB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 9.2/10.4 MB 157.3 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 9.2/10.4 MB 157.3 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 9.2/10.4 MB 157.3 kB/s eta 0:00:08\n",
      "   ------------------------------------ --- 9.4/10.4 MB 163.0 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 9.4/10.4 MB 163.0 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 9.4/10.4 MB 163.0 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 9.4/10.4 MB 163.0 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 9.4/10.4 MB 163.0 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 9.4/10.4 MB 163.0 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 9.7/10.4 MB 167.5 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 9.7/10.4 MB 167.5 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 9.7/10.4 MB 167.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.4 MB 171.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.0/10.4 MB 171.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.0/10.4 MB 171.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.0/10.4 MB 171.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.0/10.4 MB 171.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.4 MB 175.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.4 MB 175.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 184.5 kB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: typing-extensions, safetensors, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 pyyaml-6.0.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3 typing-extensions-4.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e23f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e7d2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RESEARCH related\\PreCog tasks\\precog_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m tokenizer = BertTokenizer.from_pretrained(model_name)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mBertModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_bert_embedding\u001b[39m(sentence):\n\u001b[32m     10\u001b[39m     inputs = tokenizer(sentence, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RESEARCH related\\PreCog tasks\\precog_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RESEARCH related\\PreCog tasks\\precog_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1826\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"He is a doctor.\"\n",
    "embedding_he_doctor = get_bert_embedding(sentence)\n",
    "print(embedding_he_doctor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38988d8",
   "metadata": {},
   "source": [
    "### Evaluate Gender Bias in Contextual Embeddings:\n",
    "The idea is to compare the embeddings of the word \"doctor\" when it's used in different contexts, such as \"He is a doctor\" and \"She is a doctor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_he = \"He is a doctor.\"\n",
    "sentence_she = \"She is a doctor.\"\n",
    "\n",
    "embedding_he = get_bert_embedding(sentence_he)\n",
    "embedding_she = get_bert_embedding(sentence_she)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine(embedding_he, embedding_she)\n",
    "print(f\"Cosine similarity between 'He is a doctor' and 'She is a doctor': {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af7ace",
   "metadata": {},
   "source": [
    "### Evaluate Other Biases:\n",
    "You can evaluate other biases by replacing gendered terms with gender-neutral terms or exploring occupations and nationalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca12ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_he_teacher = \"He is a teacher.\"\n",
    "sentence_she_teacher = \"She is a teacher.\"\n",
    "\n",
    "embedding_he_teacher = get_bert_embedding(sentence_he_teacher)\n",
    "embedding_she_teacher = get_bert_embedding(sentence_she_teacher)\n",
    "\n",
    "similarity_teacher = cosine(embedding_he_teacher, embedding_she_teacher)\n",
    "print(f\"Cosine similarity for teacher role: {similarity_teacher}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423fb76",
   "metadata": {},
   "source": [
    "### Visualize Contextual Bias: \n",
    "Similar to static embeddings, you can use t-SNE to visualize how contextual embeddings vary across different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embeddings = [embedding_he, embedding_she, embedding_he_teacher, embedding_she_teacher]\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n",
    "plt.annotate(\"He is a doctor\", (tsne_result[0, 0], tsne_result[0, 1]))\n",
    "plt.annotate(\"She is a doctor\", (tsne_result[1, 0], tsne_result[1, 1]))\n",
    "plt.annotate(\"He is a teacher\", (tsne_result[2, 0], tsne_result[2, 1]))\n",
    "plt.annotate(\"She is a teacher\", (tsne_result[3, 0], tsne_result[3, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196312c1",
   "metadata": {},
   "source": [
    "## Key Insights:\n",
    "This analysis will help you understand if and how BERT captures harmful biases in different contexts.\n",
    "\n",
    "You can use this to discuss why even contextual embeddings are not completely free from harmful biases, and suggest methods for reducing these biases (e.g., bias correction during training or data augmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd86663",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "#### This part of the task will involve using both static embeddings (like Word2Vec/GloVe) and contextual embeddings (like BERT) to evaluate harmful associations and biases in word representations. By quantifying these biases, you can explore strategies to mitigate them and ensure that your NLP models do not propagate harmful stereotypes.\n",
    "\n",
    "#### For both static and contextual embeddings, you can visualize results and assess which biases are most prevalent in the embeddings. The bonus task provides a critical perspective on bias and fairness in NLP and will be crucial in building more ethical AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
