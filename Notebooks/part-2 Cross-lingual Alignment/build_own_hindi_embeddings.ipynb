{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f7062a",
   "metadata": {},
   "source": [
    "# Cross-Lingual Alignment (English ↔ Hindi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03524d9",
   "metadata": {},
   "source": [
    "## Preprocess Hindi Corpus\n",
    "You can use the indic-nlp-library for Hindi-specific tokenization and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa53344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_13044\\2208323636.py:2: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  os.environ[\"INDIC_RESOURCES_PATH\"] = \"D:\\RESEARCH related\\PreCog tasks\\indic_nlp_resources\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"INDIC_RESOURCES_PATH\"] = \"D:\\RESEARCH related\\PreCog tasks\\indic_nlp_resources\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd5698",
   "metadata": {},
   "source": [
    "## Preprocessing of Hindi language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791bc08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting indic-nlp-library\n",
      "  Using cached indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Using cached sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Using cached Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from indic-nlp-library) (2.2.3)\n",
      "Requirement already satisfied: numpy in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from pandas->indic-nlp-library) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from pandas->indic-nlp-library) (2025.2)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinx-8.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting Jinja2>=3.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: Pygments>=2.17 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting babel>=2.13 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting requests>=2.30.0 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting roman-numerals-py>=1.0.0 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached roman_numerals_py-3.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: packaging>=23.0 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in d:\\research related\\precog tasks\\precog_venv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "Using cached Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Using cached sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
      "Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Using cached sphinx-8.2.3-py3-none-any.whl (3.6 MB)\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Using cached alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/10.2 MB 12.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/10.2 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/10.2 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.2 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.2 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/10.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.5/10.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.5/10.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.0/10.2 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.2/10.2 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.5/10.2 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.5/10.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.8/10.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.8/10.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.0/10.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.0/10.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.0/10.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.0/10.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.3/10.2 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.3/10.2 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.3/10.2 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 6.8/10.2 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.1/10.2 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.6/10.2 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.1/10.2 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.7/10.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/10.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.2/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.2 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.2 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 1.0 MB/s eta 0:00:00\n",
      "Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached roman_numerals_py-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: snowballstemmer, morfessor, urllib3, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, roman-numerals-py, MarkupSafe, imagesize, idna, docutils, charset-normalizer, certifi, babel, alabaster, requests, Jinja2, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed Jinja2-3.1.6 MarkupSafe-3.0.2 alabaster-1.0.0 babel-2.17.0 certifi-2025.1.31 charset-normalizer-3.4.1 docutils-0.21.2 idna-3.10 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 requests-2.32.3 roman-numerals-py-3.1.0 snowballstemmer-2.2.0 sphinx-8.2.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 urllib3-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6254f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 1\t⏺️ 03 मजदूरों को बेहतर इलाज के लिए रायपुर ले जाने की करवाई गई व्यवस्था pic.\n",
      "\n",
      "Processed: ['1', '⏺️', '03', 'मजदूरों', 'को', 'बेहतर', 'इलाज', 'के', 'लिए', 'रायपुर', 'ले', 'जाने', 'की', 'करवाई', 'गई', 'व्यवस्था', 'pic', '.']\n",
      "\n",
      "Original: 2\t• 06.00 PM से 07.00 PM: सांस्कृतिक कार्यक्रमों में हिस्सा.\n",
      "\n",
      "Processed: ['2', '•', '06.00', 'pm', 'से', '07.00', 'pm', ':', 'सांस्कृतिक', 'कार्यक्रमों', 'में', 'हिस्सा', '.']\n",
      "\n",
      "Original: 3\t० में कहा कि लॉकडाउन के बाद गरीब कल्याण योजना का ऐलान किया गया था।\n",
      "\n",
      "Processed: ['3', '०', 'में', 'कहा', 'कि', 'लॉकडाउन', 'के', 'बाद', 'गरीब', 'कल्याण', 'योजना', 'का', 'ऐलान', 'किया', 'गया', 'था', '।']\n",
      "\n",
      "Original: 4\t\"100 मरीजों पर नियंत्रित क्लिनिकल ट्रायल किया गया, जिसमें तीन दिन के अंदर 69 प्रतिशत और चार दिन के अंदर शत प्रतिशत मरीज ठीक हो गए और उनकी जांच रिपोर्ट निगेटिव आई।\"\n",
      "\n",
      "Processed: ['4', '\"', '100', 'मरीजों', 'पर', 'नियंत्रित', 'क्लिनिकल', 'ट्रायल', 'किया', 'गया', ',', 'जिसमें', 'तीन', 'दिन', 'के', 'अंदर', '69', 'प्रतिशत', 'और', 'चार', 'दिन', 'के', 'अंदर', 'शत', 'प्रतिशत', 'मरीज', 'ठीक', 'हो', 'गए', 'और', 'उनकी', 'जांच', 'रिपोर्ट', 'निगेटिव', 'आई', '।', '\"']\n",
      "\n",
      "Original: 5\t'100 में 70 अफ़सर बनने लायक़ नहीं'\n",
      "\n",
      "Processed: ['5', \"'\", '100', 'में', '70', 'अफ़सर', 'बनने', 'लायक़', 'नहीं', \"'\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import indicnlp\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set up Indic NLP resources\n",
    "INDIC_RESOURCES_PATH = \"D:\\\\RESEARCH related\\\\PreCog tasks\\\\indic_nlp_resources\"  # Replace with your path\n",
    "common.set_resources_path(INDIC_RESOURCES_PATH)\n",
    "loader.load()\n",
    "\n",
    "# Load Hindi corpus file\n",
    "path = \"D:\\\\RESEARCH related\\\\PreCog tasks\\\\Language_representations\\\\Data\\\\hin_news_2020_300K\\\\hin_news_2020_300K-sentences.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    hindi_lines = f.readlines()\n",
    "\n",
    "# Preprocess Hindi text\n",
    "def preprocess_hindi(text):\n",
    "    normalizer = IndicNormalizerFactory().get_normalizer(\"hi\") # Hindi language\n",
    "    text = normalizer.normalize(text) # Normalize the text\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with spaces\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(text, lang='hi')) # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.strip()]  # Remove empty tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_hindi = [preprocess_hindi(sent) for sent in hindi_lines]\n",
    "\n",
    "# Example usage\n",
    "for i in range(5):\n",
    "    print(f\"Original: {hindi_lines[i]}\")\n",
    "    print(f\"Processed: {processed_hindi[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dff981",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Get Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c2e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten and count\n",
    "flat_tokens = [token for sent in processed_hindi for token in sent]\n",
    "vocab_counter = Counter(flat_tokens)\n",
    "\n",
    "# Top-N words\n",
    "top_n = 10000\n",
    "vocab = [word for word, freq in vocab_counter.most_common(top_n)]\n",
    "word2id = {word: idx for idx, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e35258",
   "metadata": {},
   "source": [
    "# Build the Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302291f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def build_cooc_matrix(sentences, word2id, window_size=4):\n",
    "    cooc_mat = np.zeros((len(word2id), len(word2id)), dtype=np.float32)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        token_ids = [word2id[w] for w in sent if w in word2id]\n",
    "        for center_pos, center_id in enumerate(token_ids):\n",
    "            start = max(0, center_pos - window_size)\n",
    "            end = min(len(token_ids), center_pos + window_size + 1)\n",
    "            for context_pos in range(start, end):\n",
    "                if context_pos == center_pos:\n",
    "                    continue\n",
    "                context_id = token_ids[context_pos]\n",
    "                cooc_mat[center_id][context_id] += 1.0\n",
    "    return cooc_mat\n",
    "\n",
    "cooc_matrix = build_cooc_matrix(processed_hindi, word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f2bf9",
   "metadata": {},
   "source": [
    "# Apply Dimensionality Reduction (SVD or NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec43d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def reduce_with_svd(cooc_matrix, dim=300):\n",
    "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
    "    return svd.fit_transform(cooc_matrix)\n",
    "\n",
    "hindi_embeddings = reduce_with_svd(cooc_matrix, dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f1061",
   "metadata": {},
   "source": [
    "# Build Final Word2Vec-like Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed801bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {idx: word for word, idx in word2id.items()}\n",
    "hindi_word2vec = {id2word[i]: vec for i, vec in enumerate(hindi_embeddings)}\n",
    "\n",
    "# save my model in pickle format\n",
    "import pickle\n",
    "with open(\"D:\\\\RESEARCH related\\\\PreCog tasks\\\\Language_representations\\\\models\\\\my_hindi_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hindi_word2vec, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
